# ARTS2019002
week2 20190721

### Algorithm

1. 代码地址:
 
 - [401-Binary Watch](https://github.com/ZeroMing/LeetCodeMing/blob/master/leetcodeoj/src/main/java/org/ming/leetcodeoj/backtracking/Leetcodeoj_401_20190717.java)
 - [784-Letter Case Permutation](https://github.com/ZeroMing/LeetCodeMing/blob/master/leetcodeoj/src/main/java/org/ming/leetcodeoj/backtracking/Leetcodeoj_784_20190717.java)


### Review

1. 关于ZooKeeper在Twitter内部企业级使用的一篇技术Blog
    
    - [原文链接](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/zookeeper-at-twitter.html)
> 笔记·心得

    1. Apache ZooKeeper是一个用于分布式协调的系统。它支持实现对分布式设置中的安全性和活跃性至关重要的各种原语和机制，例如分布式锁定，主选举，组成员身份和配置管理.
    2. ZooKeeper在Twitter上用作存储关键元数据的真实来源。它作为协调内核提供分布式协调服务，例如领导者选举和分布式锁定。
        ZooKeeper的一些具体实例包括：
        - ZooKeeper用于存储服务注册表，Twitter的命名服务使用该服务注册表进行服务发现。
        - Twitter的内部键值数据库曼哈顿将其集群拓扑信息存储在ZooKeeper中。
        - EventBus是Twitter的pub-sub消息系统，它将关键元数据存储在ZooKeeper中，并使用ZooKeeper进行领导者选举。
        - Twitter的计算平台Mesos使用ZooKeeper进行领导者选举。
        当以这种方式使用ZooKeeper时，客户端很快就会遇到可扩展性瓶颈，从而迫使出现不同的解决方案。
        我们的经验表明，ZooKeeper仅用于存储少量元数据时表现最佳，并且主要从客户端应用程序的性能关键路径中提供读取工作负载
    3. 像Twitter上的大多数其他服务一样，如前所述，ZooKeeper大规模运行。我们运行许多ZooKeeper集群（称为“集合”），每个集合都有许多服务器。但是，
        与许多更高级别的服务不同，ZooKeeper是有状态的，扩展有状态服务并非易事。
    4. 瓶颈1 - 会话扩展:
        - 会话封装客户端和服务器之间的状态。会话是扩展ZooKeeper的第一个瓶颈，
        因为建立和维护会话的成本并非微不足道。每个会话建立和删除都是一个写请求，必须通过共识管道。ZooKeeper不能很好地扩展写入工作负载，
        并且每个ZooKeeper主机上有数十万个客户端，单独的会话建立请求会使整体保持忙碌，从而阻止它提供通常的读取请求。
        - 解决方案: 使用 [本地会话功能](https://issues.apache.org/jira/browse/ZOOKEEPER-1147) 解决了这个问题  面向类似问题的Facebook工程师介绍。
        目前在所有Twitter的集合中，默认使用本地会话，
        并在需要时自动升级到全局会话（例如，客户端创建短暂的节点）。
    5、 瓶颈2 - 为数十万个客户端提供读取请求
        - 在ZooKeeper中，每个客户端都需要与服务于其请求的主机保持TCP连接。鉴于客户端数量不断增加，主机迟早会达到TCP连接限制。
        - 解决: 使用ZK的 [Observer](https://zookeeper.apache.org/doc/r3.4.13/zookeeperObservers.html)。它允许在不增加共识工作量的情况下扩展读取请求的容量。为了进一步提高我们合奏的稳定性，我们通常只允许Obse​​rvers通过从DNS中删除法定成员的记录来提供直接的客户流量
    6. 其他扩展: [混合的读/写工作负载改进](https://issues.apache.org/jira/browse/ZOOKEEPER-2024)
    7. 关于ZK的运营的几点:
        - 可观察性。良好的可观察性以及对合奏的多种洞察力是必须的。我们不能将ZooKeeper作为黑盒子来操作; 
        如此复杂的系统需要良好的可见性，以便我们了解事故发生的原因。
        为了深入了解集合主机的状态，我们向ZooKeeper添加了许多指标，
        这些指标通过Finagle发布到Twitter的可观察性堆栈
        这些指标与我们的警报系统集成在一起，该警报系统将通知工程师各种级别的警报严重性。
        - 拓扑变化：集合的增长和缩小必须易于操作，对生产流量影响很小，并且安全。
        在继续提供流量的同时，我们如何更改ZooKeeper集合本身的配置？ 
        [动态重新配置](https://zookeeper.apache.org/doc/r3.5.4-beta/zookeeperReconfig.html) 在这里提供帮助。这是ZooKeeper 3.x中我们最喜欢的功能之一。
        我们构建了用于围绕动态重新配置执行拓扑更改的工具。
        这些工具可以节省大量时间，并且可以避免由于执行这些更改的旧方法而导致的中断，通过滚动重新启动。
        - 备份：必须备份数据，以便我们可以从总体集合丢失或意外删除中恢复集合。
        我们通过将本地存储的快照和事务日志推送到远程存储（在我们的示例中为HDFS）
        并将它们用作还原源来进行备份。
        备份模块在ZooKeeper代码库中编码为相对独立的模块，备份作为专用观察器运行。
    8. 下一步改进:
        - 提高ZooKeeper整体的可用性，包括改善领导者选举时间，增加速率限制，
        以便整体可以在高流量下优雅地降级，并最终支持多租户，这
        样我们就可以巩固我们的ZooKeeper整体和使其更具成本效益。
       
    9. 分布式领域中存在CAP理论，且该理论已被证明：任何分布式系统只可同时满足两点，无法三者兼顾。
        - C：Consistency，一致性，数据一致更新，所有数据变动都是同步的。
        - A：Availability，高可用性，系统具有好的响应性能。
        - P：Partition tolerance，分区容错性。
    在CAP理论中，ZK保证的CP 。所有ZK不适合做服务发现这种需要保证CA的场景中。
    一个Service发现服务应该从一开始就被设计成高可用的才行。
    
[参考链接](https://blog.csdn.net/paincupid/article/details/80610441)
    

### Tip

> 工作心经:

1. 每天把自己的时间分为几个时间段，什么时间段需要做什么，会事半功倍，便于复盘。

> 技术心经:

1. 关于git Commit message 与 Git Hooks 的最佳实践
   
   也是由于工作过程中，发现团队项目上的git提交的日志，没有统一的规范，有些提交信息简单得不明所述，
而有的提交信息太啰嗦。导致提交日志很混乱。因为之前没有很在意 Commit message的规范。
   现在简答列出几点:
   - 目前使用最广泛的提交规范: 「Angular 规范」
   - 首行作为「总结行」，简明扼要的总结提交内容,「总结行」行文使用「祈使句」,不在「总结行」结尾使用标点符号,标题性质
   - 使用「正文」来描述提交细节信息，
   -「总结行」与「正文」之间使用「空行分隔」
   -「正文」中的不同段落使用空行分隔,「正文」每一行的长度控制在 72 个字符以内
   - ***确保你的每次提交是「原子提交」，避免版本回退，导致项目报错。(最重要的一点)***


> 生活心经

1. 毁掉一个人的最好方式就是让他追求完美和达到极致。 - 白岩松


### Share

1. [Git最佳实践之 - Commit message](http://note.youdao.com/noteshare?id=8f67a6cca08c2ffdd8f0794a23f2f979&sub=25937C9B28884B46B29D715D466E1861)

  
    
